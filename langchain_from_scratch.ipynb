{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import arxiv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf hub model\n",
    "hf_hub_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "hf_chat_model = ChatHuggingFace(llm=hf_hub_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI( model_kwargs={\"stop\": [\"Observation:\"]},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A- ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"System: This is a conversation between a software engineer and intellectual AI bot. AI bot is talkative and teaches concepts to the engineer.\n",
    "Current Conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "history = ''\n",
    "\n",
    "def talk(question):\n",
    "\n",
    "    global history\n",
    "    \n",
    "    prompt_after_formatting = prompt.format(input=question,\n",
    "                                            history=history)\n",
    "\n",
    "    output = chat_model.invoke(prompt_after_formatting)\n",
    "\n",
    "    history += f\"Human: {question}\\nAI: {output.content}\\n\"\n",
    "\n",
    "    return output.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"How are you today?\")\n",
    "talk(\"Sorry. What did I ask you?\")\n",
    "talk(\"Do you know anything about football?\")\n",
    "print(prompt.format(input=\"current question\",history=history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"System: This is a conversation between a software engineer and intellectual AI bot. AI bot is talkative and teaches concepts to the engineer.\n",
    "Current Conversation Summary:\n",
    "{summary}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\"System: Your task is to summarize below conversation with emphasis on key points. If nothing given, return ''.\n",
    "{history}\n",
    "Summary:\"\"\"\n",
    "\n",
    "history = ''\n",
    "\n",
    "def talk(question):\n",
    "\n",
    "    global history\n",
    "    \n",
    "    summary = chat_model.invoke(summary_prompt.format(history=history)).content\n",
    "\n",
    "    prompt_after_formatting = prompt.format(input=question,\n",
    "                                            summary=summary)\n",
    "\n",
    "    output = chat_model.invoke(prompt_after_formatting)\n",
    "\n",
    "    history += f\"Human: {question}\\nAI: {output.content}\\n\"\n",
    "\n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"Sorry. What did I ask you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"do you know about football?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"when did england compete in euros first?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"what were the other teams in the same competition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function-Use (Agents and Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_search(args):\n",
    "    # Construct the default API client.\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the paper with ID\n",
    "    search_by_id = arxiv.Search(query=args['query'],\n",
    "                                id_list=[args['id']])\n",
    "    # Reuse client to fetch the paper, then print its title.\n",
    "    first_result = next(client.results(search_by_id))\n",
    "    \n",
    "    return first_result.title\n",
    "\n",
    "def convert_time(args):\n",
    "\n",
    "    temp = args['time'].split(':')\n",
    "\n",
    "    return int(temp[0])*3600 + int(temp[1])*60 + int(temp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "ALWAYS use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do. only one action at a time.\n",
    "Action: the action to take, should be one of {tool_names}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer.\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "tools = {\n",
    "    'convert_time': 'A function to convert a time string with format H:MM:SS to seconds, args: {\"time\": {\"type\": \"string\"}}',\n",
    "    'arxiv_search': 'A function to get information about a scientific article or articles, args: {\"query\": {\"type\": \"string\"}, \"id\": {\"type\": \"string\"}}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ''\n",
    "agent_scratchpad = ''\n",
    "\n",
    "def ask(question):\n",
    "\n",
    "    global history, agent_scratchpad\n",
    "\n",
    "    tools_str = ''\n",
    "\n",
    "    for func, desc in tools.items():\n",
    "        tools_str += f\"{func} : {desc}\\n\"\n",
    "    \n",
    "    prompt_after_formatting = system_prompt.format(input=question, \n",
    "                                                   agent_scratchpad=agent_scratchpad, \n",
    "                                                   tools=tools_str, \n",
    "                                                   tool_names=list(tools.keys()))\n",
    "    print(prompt_after_formatting)\n",
    "    output = chat_model.invoke(prompt_after_formatting)\n",
    "    print(output.content)\n",
    "    \n",
    "    while output.content.find(\"Final Answer: \")==-1:\n",
    "\n",
    "        agent_scratchpad += output.content\n",
    "        #history += f\"Human: {question}\\nAI: {output.content}\\n\"\n",
    "\n",
    "        function_name = output.content.split(\"Action: \")[1].split('\\n')[0]\n",
    "        function_args = ast.literal_eval(output.content.split(\"Action Input: \")[1].split('\\n')[0])\n",
    "\n",
    "        result = globals()[function_name](function_args)\n",
    "        agent_scratchpad += f\"Observation: {result}\\n\"\n",
    "        \n",
    "        prompt_after_formatting = system_prompt.format(input=question, \n",
    "                                                    agent_scratchpad=agent_scratchpad, \n",
    "                                                    tools=tools_str, \n",
    "                                                    tool_names=list(tools.keys()))\n",
    "\n",
    "        output = chat_model.invoke(prompt_after_formatting)\n",
    "        print(output.content)\n",
    "    \n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask(\"First answer how many seconds in 4:00:01. Then answer What's the paper 1605.08386 about?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
