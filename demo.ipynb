{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect 5 tools: weather + kg frus rag + convert time + arxiv + serpapi\n",
    "# allow daily conversation beyond tool use (+ memory)\n",
    "\n",
    "# todo:\n",
    "# 1- add conversation memory to agent (make sure it chats beyond tool-use)\n",
    "# 2- add rag\n",
    "# 3- add serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import arxiv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model_kwargs={\"stop\": [\"Observation:\"]},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "file_path = (\n",
    "    \"../../eth_courses/thesis/gokberk_thesis_report_04_06_23.pdf\"\n",
    ")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    all_splits,\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    ")\n",
    "\n",
    "prompt = \"\"\"System: You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question which might reference context in the chat history, \n",
    "formulate a standalone question which can be understood without the chat history. \n",
    "Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\n",
    "If chat history is empty (''), do NOT change anything in the question.\n",
    "\n",
    "Chat history:\n",
    "{history}\n",
    "Question:\n",
    "{input}\n",
    "Reformulated Question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_search(args):\n",
    "    # Construct the default API client.\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the paper with ID\n",
    "    search_by_id = arxiv.Search(query=args['query'],\n",
    "                                id_list=[args['id']])\n",
    "    # Reuse client to fetch the paper, then print its title.\n",
    "    first_result = next(client.results(search_by_id))\n",
    "    \n",
    "    return first_result.title\n",
    "\n",
    "def convert_time(args):\n",
    "\n",
    "    temp = args['time'].split(':')\n",
    "\n",
    "    return int(temp[0])*3600 + int(temp[1])*60 + int(temp[2])\n",
    "\n",
    "\n",
    "def kgfrus_search(args):\n",
    "\n",
    "    global history\n",
    "\n",
    "    rephrased_q = chat_model.invoke(contextualize_q_system_prompt.format(input=args['question'],history=history)).content\n",
    "    print(rephrased_q)\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in vectorstore.similarity_search(rephrased_q)])\n",
    "\n",
    "    prompt_after_formatting = prompt.format(input=rephrased_q,\n",
    "                                            context=context)\n",
    "    print(prompt_after_formatting)\n",
    "    output = chat_model.invoke(prompt_after_formatting)\n",
    "\n",
    "    history += f\"Human: {args['question']}\\nAI: {output.content}\\n\"\n",
    "\n",
    "    return output.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "ALWAYS use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do. only one action at a time.\n",
    "Action: the action to take, should be one of {tool_names}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer.\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "tools = {\n",
    "    'convert_time': 'A function to convert a time string with format H:MM:SS to seconds, args: {\"time\": {\"type\": \"string\"}}',\n",
    "    'arxiv_search': 'A function to get information about a scientific article or articles, args: {\"query\": {\"type\": \"string\"}, \"id\": {\"type\": \"string\"}}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ''\n",
    "agent_scratchpad = ''\n",
    "\n",
    "def ask(question):\n",
    "\n",
    "    global history, agent_scratchpad\n",
    "\n",
    "    tools_str = ''\n",
    "\n",
    "    for func, desc in tools.items():\n",
    "        tools_str += f\"{func} : {desc}\\n\"\n",
    "    \n",
    "    prompt_after_formatting = system_prompt.format(input=question, \n",
    "                                                   agent_scratchpad=agent_scratchpad, \n",
    "                                                   tools=tools_str, \n",
    "                                                   tool_names=list(tools.keys()))\n",
    "    print(prompt_after_formatting)\n",
    "    output = chat_model.invoke(prompt_after_formatting)\n",
    "    print(output.content)\n",
    "    \n",
    "    while output.content.find(\"Final Answer: \")==-1:\n",
    "\n",
    "        agent_scratchpad += output.content\n",
    "        #history += f\"Human: {question}\\nAI: {output.content}\\n\"\n",
    "\n",
    "        function_name = output.content.split(\"Action: \")[1].split('\\n')[0]\n",
    "        function_args = ast.literal_eval(output.content.split(\"Action Input: \")[1].split('\\n')[0])\n",
    "\n",
    "        result = globals()[function_name](function_args)\n",
    "        agent_scratchpad += f\"Observation: {result}\\n\"\n",
    "        \n",
    "        prompt_after_formatting = system_prompt.format(input=question, \n",
    "                                                    agent_scratchpad=agent_scratchpad, \n",
    "                                                    tools=tools_str, \n",
    "                                                    tool_names=list(tools.keys()))\n",
    "\n",
    "        output = chat_model.invoke(prompt_after_formatting)\n",
    "        print(output.content)\n",
    "    \n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "convert_time : A function to convert a time string with format H:MM:SS to seconds, args: {\"time\": {\"type\": \"string\"}}\n",
      "arxiv_search : A function to get information about a scientific article or articles, args: {\"query\": {\"type\": \"string\"}, \"id\": {\"type\": \"string\"}}\n",
      "\n",
      "\n",
      "ALWAYS use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do. only one action at a time.\n",
      "Action: the action to take, should be one of ['convert_time', 'arxiv_search']\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer.\n",
      "\n",
      "Question: First answer how many seconds in 4:00:01. Then answer What's the paper 1605.08386 about?\n",
      "Thought:\n",
      "We need to convert the time 4:00:01 to seconds first, then we can search for the paper with ID 1605.08386.\n",
      "Action: convert_time\n",
      "Action Input: {\"time\": \"4:00:01\"}\n",
      "\n",
      "Action: arxiv_search\n",
      "Action Input: {\"query\": \"\", \"id\": \"1605.08386\"}\n",
      "\n",
      "Final Answer: 14401 seconds. The paper with ID 1605.08386 is about \"Heat-bath random walks with Markov bases\".\n",
      "Final Answer: 14401 seconds. The paper with ID 1605.08386 is about \"Heat-bath random walks with Markov bases\".\n"
     ]
    }
   ],
   "source": [
    "print(ask(\"First answer how many seconds in 4:00:01. Then answer What's the paper 1605.08386 about?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
